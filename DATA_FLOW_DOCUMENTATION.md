# SignLingo Bridge - Complete Data Flow Documentation

## ğŸŒŠ Overview: How Data Flows Through Your App

This document explains the complete journey of data from camera capture to final translated output.

---

## ğŸ“Š High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER'S BROWSER                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    FRONTEND (React)                       â”‚ â”‚
â”‚  â”‚  Camera â†’ MediaPipe â†’ Canvas â†’ WebSocket â†’ UI Display    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†• WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     BACKEND SERVER (Python)                     â”‚
â”‚               Sign Language ML Model (FastAPI)                  â”‚
â”‚            ws://127.0.0.1:8000/ws (WebSocket)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†• HTTPS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXTERNAL APIs (Cloud)                        â”‚
â”‚  â€¢ Google Translate (Translation)                              â”‚
â”‚  â€¢ MyMemory (Translation Backup)                               â”‚
â”‚  â€¢ LanguageTool (Grammar Correction)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Complete Data Flow - Step by Step

### **PHASE 1: Camera Capture & Hand Detection** ğŸ“¹

#### Step 1.1: Camera Access

```
User clicks "Start" button
         â†“
Index.tsx: startRecognition()
         â†“
navigator.mediaDevices.getUserMedia()
         â†“
Camera stream activated (front/back camera)
         â†“
Video element receives stream
         â†“
<video ref={videoRef}> displays live feed
```

**File:** `src/pages/Index.tsx` (lines ~148-200)

**Data Structure:**

```javascript
MediaStream {
  video: true,
  facingMode: "user" or "environment"  // Front or back camera
}
```

---

#### Step 1.2: MediaPipe Initialization

```
initializeMediaPipe() called
         â†“
Load MediaPipe Hands WASM model
         â†“
Configure hand detection settings:
  - maxNumHands: 2
  - modelComplexity: 1
  - minDetectionConfidence: 0.5
  - minTrackingConfidence: 0.5
         â†“
handsRef.current = Hands instance
         â†“
cameraRef.current = Camera instance
         â†“
Camera starts sending frames to MediaPipe
```

**File:** `src/pages/Index.tsx` (lines ~48-54)

**Data Structure:**

```javascript
Hands {
  maxNumHands: 2,
  modelComplexity: 1,
  minDetectionConfidence: 0.5,
  minTrackingConfidence: 0.5,
  onResults: onResults callback
}
```

---

### **PHASE 2: Hand Landmark Detection** ğŸ–ï¸

#### Step 2.1: MediaPipe Processing

```
Camera captures frame (60 FPS)
         â†“
MediaPipe Hands processes frame
         â†“
Detects hands in image
         â†“
Extracts 21 landmarks per hand
         â†“
Identifies handedness (Left/Right)
         â†“
Calls onResults() with detection data
```

**File:** `src/pages/Index.tsx` (lines ~55-105)

**Data Structure:**

```javascript
results = {
  image: VideoFrame,
  multiHandLandmarks: [
    [
      { x: 0.5, y: 0.3, z: -0.1 }, // Landmark 0 (wrist)
      { x: 0.52, y: 0.25, z: -0.08 }, // Landmark 1 (thumb base)
      // ... 19 more landmarks per hand
    ],
  ],
  multiHandedness: [
    {
      index: 0,
      label: "Right", // or "Left"
      score: 0.95, // confidence
    },
  ],
};
```

---

#### Step 2.2: Canvas Display with Mirroring

```
onResults() receives detection
         â†“
Get canvas context
         â†“
Apply horizontal flip transform:
  ctx.translate(width, 0)
  ctx.scale(-1, 1)
         â†“
Draw mirrored video frame
         â†“
Draw hand landmarks (circles)
         â†“
Draw hand connections (lines)
         â†“
Draw handedness label (Left=purple, Right=blue)
         â†“
User sees non-mirrored display
```

**File:** `src/pages/Index.tsx` (lines ~55-90)

**Visual Transformation:**

```javascript
// Original camera: ğŸ‘ˆ (mirrored - wrong)
// After transform: ğŸ‘‰ (correct - matches real world)

ctx.translate(w, 0); // Move origin to right edge
ctx.scale(-1, 1); // Flip horizontally
```

---

### **PHASE 3: Data Preparation for Backend** ğŸ“¦

#### Step 3.1: Landmark Transformation

```
For each detected hand:
         â†“
Get original handedness label
         â†“
Flip handedness for display:
  originalHandedness === "Right" ? "Left" : "Right"
         â†“
Get original landmarks
         â†“
Flip landmark X-coordinates:
  flippedLandmarks = landmarks.map(lm => ({
    ...lm,
    x: 1 - lm.x  // Flip horizontally
  }))
         â†“
Store in handsData array
```

**File:** `src/pages/Index.tsx` (lines ~91-105)

**Data Structure:**

```javascript
handsData = [
  {
    handedness: "Left", // Display handedness (flipped)
    landmarks: [
      { x: 0.5, y: 0.3, z: -0.1 }, // Flipped x-coordinate
      { x: 0.48, y: 0.25, z: -0.08 },
      // ... 21 landmarks total
    ],
  },
];
```

**Why flip?** Model was trained on mirrored camera data, so we send flipped coordinates to match training format.

---

#### Step 3.2: WebSocket Transmission

```
handsData prepared
         â†“
Check if WebSocket connected:
  wsRef.current?.readyState === WebSocket.OPEN
         â†“
Convert to JSON string
         â†“
Send via WebSocket:
  ws.send(JSON.stringify({ hands: handsData }))
         â†“
Data travels to backend server
```

**File:** `src/pages/Index.tsx` (lines ~102-105)

**Message Format:**

```json
{
  "hands": [
    {
      "handedness": "Left",
      "landmarks": [
        { "x": 0.5, "y": 0.3, "z": -0.1 },
        { "x": 0.48, "y": 0.25, "z": -0.08 }
        // ... 21 landmarks
      ]
    }
  ]
}
```

---

### **PHASE 4: Backend Processing** ğŸ¤–

#### Step 4.1: WebSocket Reception

```
Backend receives WebSocket message
         â†“
Parse JSON data
         â†“
Extract hand landmarks
         â†“
Preprocess landmarks:
  - Normalize coordinates
  - Calculate angles
  - Extract features
         â†“
Feed to ML model
```

**Backend Server:** `ws://127.0.0.1:8000/ws` (Your Python backend)

**Expected Backend Data Flow:**

```python
# Pseudo-code (your backend implementation)
async def websocket_endpoint(websocket):
    data = await websocket.receive_json()
    hands = data['hands']

    for hand in hands:
        landmarks = hand['landmarks']
        handedness = hand['handedness']

        # Preprocess
        features = preprocess_landmarks(landmarks)

        # Predict letter
        letter = letter_model.predict(features)

        # Build word
        current_word += letter

        # Detect space (pause) â†’ complete word
        if detect_pause():
            # Apply spell-checker (THIS IS WHERE "jenisha" â†’ "geisha" HAPPENS!)
            corrected_word = spell_check(current_word)

            # Add to sentence
            sentence += corrected_word + " "
            current_word = ""

    # Send predictions back to frontend
    await websocket.send_json({
        'type': 'prediction',
        'data': {
            'letter': letter,
            'word': corrected_word,
            'sentence': sentence
        }
    })
```

---

#### Step 4.2: ML Model Prediction

```
Preprocessed features
         â†“
Sign Language Recognition Model
  (Trained on ASL/ISL dataset)
         â†“
Predicts letter (A-Z)
         â†“
Confidence score calculated
         â†“
Letter added to current word
         â†“
Check for space gesture (pause)
         â†“
If space detected:
  - Complete current word
  - Apply spell-checker âš ï¸ (Problem source!)
  - Add to sentence
         â†“
Return predictions
```

**Model Architecture (typical):**

```
Input: 21 landmarks Ã— 3 coordinates = 63 features
         â†“
Dense layers / CNN / LSTM
         â†“
Softmax output: 26 classes (A-Z) + space
         â†“
Letter prediction with confidence
```

---

#### Step 4.3: Backend Response

```
Prepare response object
         â†“
Apply spell-checker to word:
  "jenisha" â†’ "geisha" âš ï¸ (Backend spell-check mistake)
         â†“
Send via WebSocket:
  ws.send_json({
    'type': 'prediction',
    'data': {
      'letter': 'a',
      'word': 'geisha',  â† Wrong word from spell-checker!
      'sentence': 'hi geisha'
    }
  })
         â†“
Data travels back to frontend
```

**Message Format:**

```json
{
  "type": "prediction",
  "data": {
    "letter": "a",
    "word": "geisha",
    "sentence": "hi geisha"
  }
}
```

---

### **PHASE 5: Frontend Reception & Correction** ğŸ”§

#### Step 5.1: WebSocket Message Reception

```
Frontend receives WebSocket message
         â†“
Parse JSON data
         â†“
Extract predictions:
  rawLetter = data.data.letter
  rawWord = data.data.word
  rawSentence = data.data.sentence
         â†“
Apply word correction:
  correctedWord = correctWord(rawWord)
  correctedSentence = correctSentence(rawSentence)
         â†“
Check correction dictionary:
  "geisha" â†’ "jenisha" âœ… (Fixed!)
         â†“
Update UI state:
  setLetter(rawLetter)
  setWord(correctedWord)
  setSentence(correctedSentence)
```

**File:** `src/pages/Index.tsx` (lines ~143-170)

**Correction Logic:**

```typescript
// Word correction dictionary
wordCorrections = {
  geisha: "jenisha", // Fixes backend mistake!
  geesha: "jenisha",
  gaisha: "jenisha",
};

// Apply correction
const correctedWord = correctWord("geisha"); // Returns: "jenisha" âœ…
```

**Console Log:**

```
ğŸ”§ Word corrected: "geisha" â†’ "jenisha"
ğŸ”§ Sentence corrected: "hi geisha" â†’ "hi jenisha"
```

---

#### Step 5.2: UI State Update

```
React state updates triggered
         â†“
setLetter("a")
setWord("jenisha")  â† Corrected!
setSentence("hi jenisha")  â† Corrected!
         â†“
React re-renders components
         â†“
UI displays updated predictions
```

**File:** `src/pages/Index.tsx` (State variables at lines ~24-27)

**State Variables:**

```typescript
const [letter, setLetter] = useState("-"); // Current letter
const [word, setWord] = useState(""); // Current word being formed
const [sentence, setSentence] = useState(""); // Complete sentence
```

---

#### Step 5.3: UI Display

```
UI components render with new data
         â†“
Current Letter Card:
  Shows: "a"
         â†“
Predicted Word Card:
  Shows: "jenisha" âœ… (corrected from "geisha")
         â†“
Full Sentence Card:
  Shows: "hi jenisha" âœ… (corrected)
         â†“
User sees corrected predictions
```

**File:** `src/pages/Index.tsx` (lines ~419-513)

**UI Component Structure:**

```jsx
<Card>Current Letter: {letter}</Card>
<Card>Predicted Word: {word}</Card>
<Card>Full Sentence: {sentence}</Card>
```

---

### **PHASE 6: Grammar Correction** âœï¸

#### Step 6.1: User Clicks "Fix Grammar"

```
User clicks "Fix Grammar" button
         â†“
fixGrammar() function called
         â†“
Get current sentence text
         â†“
Check if text exists
         â†“
Set loading state: isCorrectingGrammar = true
         â†“
Show toast: "Checking grammar..."
         â†“
Call autoCorrect(sentence, language)
```

**File:** `src/pages/Index.tsx` (lines ~359-392)

**Trigger:**

```tsx
<Button onClick={fixGrammar}>Fix Grammar</Button>
```

---

#### Step 6.2: Grammar API Call

```
autoCorrect() calls checkGrammar()
         â†“
Prepare form data:
  text = "hi jenisha i like learn sign language"
  language = "en-US" or "hi-IN"
         â†“
POST to LanguageTool API:
  https://api.languagetool.org/v2/check
         â†“
Send URLSearchParams:
  text=...&language=en-US
         â†“
LanguageTool analyzes text
         â†“
Returns grammar matches (errors)
```

**File:** `src/lib/grammar.ts` (lines ~12-35)

**API Request:**

```javascript
POST https://api.languagetool.org/v2/check
Content-Type: application/x-www-form-urlencoded

text=hi jenisha i like learn sign language&language=en-US
```

**API Response:**

```json
{
  "matches": [
    {
      "message": "Possible agreement error",
      "offset": 15,
      "length": 11,
      "replacements": [{ "value": "like to learn" }]
    },
    {
      "message": "Capitalization",
      "offset": 0,
      "length": 2,
      "replacements": [{ "value": "Hi" }]
    }
  ]
}
```

---

#### Step 6.3: Apply Corrections

```
Receive grammar matches
         â†“
Sort matches by offset (descending)
         â†“
For each match (work backwards):
  - Extract text before error
  - Extract text after error
  - Get replacement from suggestions
  - Rebuild: before + replacement + after
         â†“
Return corrected text
         â†“
Update sentence state
         â†“
Show toast: "Grammar corrected!"
```

**File:** `src/lib/grammar.ts` (lines ~38-56)

**Correction Example:**

```
Original: "hi jenisha i like learn sign language"
            â†“
Match 1: "like learn" â†’ "like to learn" (offset: 15)
Result: "hi jenisha i like to learn sign language"
            â†“
Match 2: "hi" â†’ "Hi" (offset: 0)
Final: "Hi jenisha i like to learn sign language" âœ…
```

---

### **PHASE 7: Translation** ğŸŒ

#### Step 7.1: User Clicks "Translate"

```
User clicks "Translate" button
         â†“
translateSentence() function called
         â†“
Get current sentence text
         â†“
Determine translation direction:
  isHindi = false â†’ Translate EN â†’ HI
  isHindi = true â†’ Translate HI â†’ EN
         â†“
Set loading state: isTranslating = true
         â†“
Show toast: "Translating to Hindi/English..."
         â†“
Call autoTranslate(sentence, isHindi)
```

**File:** `src/pages/Index.tsx` (lines ~327-357)

**Trigger:**

```tsx
<Button onClick={translateSentence}>
  {isTranslating ? "Translating..." : "Translate"}
</Button>
```

---

#### Step 7.2: Google Translate (Primary)

```
autoTranslate() calls translate()
         â†“
Try Google Translate first:
  googleTranslate(text, 'en', 'hi')
         â†“
Build Google API URL:
  https://translate.googleapis.com/translate_a/single
  ?client=gtx&sl=en&tl=hi&dt=t&q=...
         â†“
Send GET request (unofficial API)
         â†“
Receive response (JSON array)
         â†“
Extract translated text
         â†“
Names automatically transliterated:
  "jenisha" â†’ "à¤œà¥‡à¤¨à¤¿à¤¶à¤¾" (Hindi script)
         â†“
Return translation
```

**File:** `src/lib/translate.ts` (lines ~15-45)

**API Request:**

```
GET https://translate.googleapis.com/translate_a/single?
    client=gtx&
    sl=en&
    tl=hi&
    dt=t&
    q=Hi%20Jenisha%20I%20like%20to%20learn%20sign%20language
```

**API Response:**

```json
[
  [
    [
      "à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤œà¥‡à¤¨à¤¿à¤¶à¤¾ à¤®à¥à¤à¥‡ à¤¸à¤¾à¤‚à¤•à¥‡à¤¤à¤¿à¤• à¤­à¤¾à¤·à¤¾ à¤¸à¥€à¤–à¤¨à¤¾ à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆ",
      "Hi Jenisha I like to learn sign language",
      null,
      null,
      3
    ]
  ],
  null,
  "en"
]
```

**Extracted Translation:**

```
"à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤œà¥‡à¤¨à¤¿à¤¶à¤¾ à¤®à¥à¤à¥‡ à¤¸à¤¾à¤‚à¤•à¥‡à¤¤à¤¿à¤• à¤­à¤¾à¤·à¤¾ à¤¸à¥€à¤–à¤¨à¤¾ à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆ"
```

---

#### Step 7.3: MyMemory Fallback (If Google Fails)

```
If Google Translate fails:
         â†“
Catch error
         â†“
Try MyMemory API:
  myMemoryTranslate(text, 'en', 'hi')
         â†“
Build MyMemory API URL:
  https://api.mymemory.translated.net/get
  ?q=...&langpair=en|hi
         â†“
Send GET request
         â†“
Receive response (JSON object)
         â†“
Extract translated text
         â†“
Return translation
```

**File:** `src/lib/translate.ts` (lines ~48-77)

**API Request:**

```
GET https://api.mymemory.translated.net/get?
    q=Hi%20Jenisha%20I%20like%20to%20learn%20sign%20language&
    langpair=en|hi
```

**API Response:**

```json
{
  "responseData": {
    "translatedText": "à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤œà¥‡à¤¨à¤¿à¤¶à¤¾ à¤®à¥à¤à¥‡ à¤¸à¤¾à¤‚à¤•à¥‡à¤¤à¤¿à¤• à¤­à¤¾à¤·à¤¾ à¤¸à¥€à¤–à¤¨à¤¾ à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆ",
    "match": 0.95
  },
  "responseStatus": 200
}
```

---

#### Step 7.4: Update UI with Translation

```
Translation received
         â†“
Update sentence state:
  setSentence(translatedText)
         â†“
Toggle language state:
  setIsHindi(!isHindi)
         â†“
Show success toast:
  "Translated to Hindi! (via Google Translate)"
         â†“
Log to console:
  Original: "Hi Jenisha..."
  Translated: "à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤œà¥‡à¤¨à¤¿à¤¶à¤¾..."
  Service: "google"
         â†“
React re-renders UI
         â†“
User sees translated text
```

**File:** `src/pages/Index.tsx` (lines ~342-353)

**Console Log:**

```
ğŸ”„ Trying Google Translate: en â†’ hi
âœ… Google Translate success
Original: Hi Jenisha I like to learn sign language
Translated: à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤œà¥‡à¤¨à¤¿à¤¶à¤¾ à¤®à¥à¤à¥‡ à¤¸à¤¾à¤‚à¤•à¥‡à¤¤à¤¿à¤• à¤­à¤¾à¤·à¤¾ à¤¸à¥€à¤–à¤¨à¤¾ à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆ
Service: Google Translate
```

---

### **PHASE 8: Speech Output (Placeholder)** ğŸ”Š

#### Step 8.1: User Clicks "Speak"

```
User clicks "Speak" button
         â†“
speakCurrent() function called
         â†“
Get current sentence text
         â†“
Check if text exists
         â†“
Show toast with preview:
  "(Placeholder) Would speak: ..."
         â†“
Log to console:
  speak placeholder {
    text: "...",
    lang: "hi-IN" or "en-US"
  }
```

**File:** `src/pages/Index.tsx` (line ~325)

**Current Implementation:**

```typescript
const speakCurrent = () => {
  const text = sentence || word || (letter === "-" ? "" : letter);
  if (!text) return toast.info("Nothing to speak");
  const preview = text.length > 120 ? text.slice(0, 117) + "..." : text;
  toast.info(`(Placeholder) Would speak: ${preview}`);
  console.log("speak placeholder", { text, lang: isHindi ? "hi-IN" : "en-US" });
};
```

**Future Implementation:**

```typescript
// Using Web Speech API
const utterance = new SpeechSynthesisUtterance(text);
utterance.lang = isHindi ? "hi-IN" : "en-US";
window.speechSynthesis.speak(utterance);
```

---

## ğŸ”„ Complete End-to-End Example

Let's trace a complete user interaction:

### **Scenario: User signs "Hi Jenisha"**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: User signs "H"                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Camera â†’ MediaPipe â†’ Detects hand shape
                  â†’ Extracts 21 landmarks
                  â†’ Flips coordinates
                  â†’ Sends to backend
Backend â†’ ML model â†’ Predicts "H"
                  â†’ Sends back: { letter: "H", word: "h", sentence: "" }
Frontend â†’ Receives message
        â†’ Displays: Letter: H, Word: h

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: User signs "I"                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Same process...
Backend â†’ Predicts "I"
       â†’ Sends: { letter: "I", word: "hi", sentence: "" }
Frontend â†’ Displays: Letter: I, Word: hi

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: User signs SPACE (pause gesture)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Backend â†’ Detects space
       â†’ Completes word: "hi"
       â†’ Adds to sentence
       â†’ Sends: { letter: " ", word: "", sentence: "hi " }
Frontend â†’ Displays: Letter: -, Word: "", Sentence: "hi"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: User signs "J-E-N-I-S-H-A"                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Backend â†’ Predicts each letter
       â†’ Builds word: "jenisha"
       â†’ After each letter: { letter: "A", word: "jenisha", ... }
Frontend â†’ Displays: Letter: A, Word: jenisha

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 5: User signs SPACE again                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Backend â†’ Detects space
       â†’ Completes word: "jenisha"
       â†’ Spell-checker runs: "jenisha" â†’ "geisha" âŒ
       â†’ Adds to sentence: "hi geisha"
       â†’ Sends: { letter: " ", word: "", sentence: "hi geisha" }
Frontend â†’ Receives: "hi geisha"
        â†’ Word correction: "geisha" â†’ "jenisha" âœ…
        â†’ Displays: Sentence: "hi jenisha" âœ…
        â†’ Console: "ğŸ”§ Sentence corrected: 'hi geisha' â†’ 'hi jenisha'"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 6: User clicks "Stop"                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Frontend â†’ stopRecognition()
        â†’ Sends stop signal to backend
        â†’ Waits 500ms for final predictions
        â†’ Receives final sentence: "hi jenisha"
        â†’ Closes WebSocket
        â†’ Stops camera
        â†’ Shows toast: "Recognition stopped"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 7: User clicks "Fix Grammar"                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Frontend â†’ Gets sentence: "hi jenisha"
        â†’ Calls LanguageTool API
LanguageTool â†’ Analyzes grammar
            â†’ Finds error: "hi" should be "Hi" (capitalization)
            â†’ Returns match: { offset: 0, replacement: "Hi" }
Frontend â†’ Applies correction
        â†’ Updates: "Hi jenisha"
        â†’ Displays corrected sentence
        â†’ Shows toast: "Grammar corrected!"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 8: User clicks "Translate"                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Frontend â†’ Gets sentence: "Hi jenisha"
        â†’ Calls Google Translate API
Google â†’ Translates EN â†’ HI
      â†’ "Hi" â†’ "à¤¨à¤®à¤¸à¥à¤¤à¥‡"
      â†’ "jenisha" â†’ "à¤œà¥‡à¤¨à¤¿à¤¶à¤¾" (transliterated)
      â†’ Returns: "à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤œà¥‡à¤¨à¤¿à¤¶à¤¾"
Frontend â†’ Updates sentence: "à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤œà¥‡à¤¨à¤¿à¤¶à¤¾"
        â†’ Toggles language: isHindi = true
        â†’ Shows toast: "Translated to Hindi! (via Google Translate)"
        â†’ Displays: "à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤œà¥‡à¤¨à¤¿à¤¶à¤¾"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 9: User clicks "Speak" (placeholder)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Frontend â†’ Gets sentence: "à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤œà¥‡à¤¨à¤¿à¤¶à¤¾"
        â†’ Shows toast: "(Placeholder) Would speak: à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤œà¥‡à¤¨à¤¿à¤¶à¤¾"
        â†’ Logs: { text: "à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤œà¥‡à¤¨à¤¿à¤¶à¤¾", lang: "hi-IN" }
        â†’ (Future: Would use speech synthesis)
```

---

## ğŸ“ Key Files & Their Roles

### **Frontend Files**

| File                        | Role             | Data Flow                       |
| --------------------------- | ---------------- | ------------------------------- |
| `src/pages/Index.tsx`       | Main component   | Orchestrates all data flow      |
| `src/lib/wordCorrection.ts` | Word correction  | Fixes backend spelling mistakes |
| `src/lib/grammar.ts`        | Grammar checking | Calls LanguageTool API          |
| `src/lib/translate.ts`      | Translation      | Calls Google/MyMemory APIs      |

### **Data Structures**

```typescript
// Camera/MediaPipe
MediaStream â†’ VideoFrame â†’ HandLandmarks

// Hand Detection
HandLandmarks = Array<{x: number, y: number, z: number}>  // 21 points
Handedness = "Left" | "Right"

// WebSocket Messages
BackendMessage = {
  type: "prediction",
  data: {
    letter: string,    // Current letter
    word: string,      // Current word being formed
    sentence: string   // Complete sentence
  }
}

// API Responses
GrammarResponse = {
  matches: Array<{
    message: string,
    offset: number,
    length: number,
    replacements: Array<{value: string}>
  }>
}

TranslationResponse = {
  translatedText: string,
  service: "google" | "mymemory",
  originalText: string
}
```

---

## ğŸ” Data Transformations

### **1. Camera to Landmarks**

```
Raw Camera Frame (1920x1080 pixels)
         â†“
MediaPipe Processing
         â†“
Normalized Landmarks (0-1 range)
[{x: 0.5, y: 0.3, z: -0.1}, ...]
```

### **2. Landmarks to Flipped Landmarks**

```
Original: x = 0.3
         â†“
Flipped: x = 1 - 0.3 = 0.7
         â†“
Sent to Backend
```

### **3. Backend Prediction to Corrected Word**

```
Backend: "geisha"
         â†“
Dictionary Lookup: wordCorrections["geisha"]
         â†“
Frontend: "jenisha" âœ…
```

### **4. English to Hindi**

```
English: "Hi Jenisha"
         â†“
Google Translate API
         â†“
Hindi: "à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤œà¥‡à¤¨à¤¿à¤¶à¤¾"
         â†“
(Transliteration: "namaste Jenisha")
```

---

## ğŸ¯ Summary: Complete Flow

```
USER SIGNS
    â†“
CAMERA CAPTURE (60 FPS)
    â†“
MEDIAPIPE DETECTION (Hand Landmarks)
    â†“
CANVAS DISPLAY (Mirrored, with Landmarks)
    â†“
WEBSOCKET SEND (Flipped Landmarks to Backend)
    â†“
BACKEND ML MODEL (Letter/Word/Sentence Prediction)
    â†“
BACKEND SPELL-CHECKER (âš ï¸ "jenisha" â†’ "geisha")
    â†“
WEBSOCKET RECEIVE (Predictions from Backend)
    â†“
WORD CORRECTION (âœ… "geisha" â†’ "jenisha")
    â†“
UI UPDATE (Display Corrected Text)
    â†“
[Optional] GRAMMAR CORRECTION (LanguageTool API)
    â†“
[Optional] TRANSLATION (Google/MyMemory API)
    â†“
[Optional] SPEECH OUTPUT (Placeholder)
    â†“
FINAL RESULT DISPLAYED TO USER
```

---

## ğŸš€ Performance Metrics

### **Latency Breakdown**

| Step                         | Time           | Notes             |
| ---------------------------- | -------------- | ----------------- |
| Camera capture               | ~16ms          | 60 FPS            |
| MediaPipe detection          | ~30-50ms       | Per frame         |
| Canvas rendering             | ~5ms           | Per frame         |
| WebSocket send               | ~1-5ms         | Local network     |
| Backend ML prediction        | ~50-100ms      | Model inference   |
| WebSocket receive            | ~1-5ms         | Local network     |
| Word correction              | <1ms           | Dictionary lookup |
| UI update                    | ~16ms          | React render      |
| **Total (camera â†’ display)** | **~120-200ms** | **Real-time!**    |

### **API Latencies**

| API              | Average Time | Notes              |
| ---------------- | ------------ | ------------------ |
| LanguageTool     | 200-400ms    | Grammar check      |
| Google Translate | 200-400ms    | Translation        |
| MyMemory         | 300-500ms    | Translation backup |

---

## ğŸ“Š Data Volume

### **Per Frame (60 FPS)**

```
1 hand = 21 landmarks Ã— 3 coordinates = 63 numbers
2 hands = 126 numbers
+ handedness labels = ~150 bytes JSON

60 FPS = 9 KB/second to backend
```

### **Per Session**

```
Average session: 2 minutes
Data sent to backend: ~1 MB
Data received from backend: ~10 KB
API calls: 0-5 (grammar/translation)
```

---

## ğŸ”’ Security & Privacy

### **Data Flow Security**

| Connection              | Protocol          | Security                  |
| ----------------------- | ----------------- | ------------------------- |
| Frontend â†” Backend      | WebSocket (ws://) | âš ï¸ Local only (localhost) |
| Frontend â†” Google       | HTTPS             | âœ… Encrypted              |
| Frontend â†” MyMemory     | HTTPS             | âœ… Encrypted              |
| Frontend â†” LanguageTool | HTTPS             | âœ… Encrypted              |

### **Data Privacy**

```
Camera feed â†’ Never leaves device (processed locally by MediaPipe)
Hand landmarks â†’ Sent to backend (your server)
Text predictions â†’ Sent to external APIs (grammar/translation)
```

**Note:** For production, upgrade WebSocket to WSS (secure WebSocket) when hosting!

---

## ğŸ“ Understanding the Architecture

### **Why This Design?**

1. **MediaPipe (Frontend)**: Fast hand detection in browser using WASM
2. **Backend ML Model**: Heavy sign language recognition model runs on server
3. **External APIs**: Leverage existing services (translation/grammar) instead of building from scratch
4. **Word Correction (Frontend)**: Quick fix for backend spelling mistakes
5. **WebSocket**: Real-time bidirectional communication (low latency)

### **Trade-offs**

| Aspect           | Decision             | Trade-off                               |
| ---------------- | -------------------- | --------------------------------------- |
| Hand detection   | Frontend (MediaPipe) | âœ… Fast, âŒ Client resources            |
| Sign recognition | Backend (Python)     | âœ… Powerful ML, âŒ Network latency      |
| Translation      | External API         | âœ… No maintenance, âš ï¸ Rate limits       |
| Word correction  | Frontend             | âœ… Quick fix, âš ï¸ Doesn't fix root cause |

---

## ğŸ”® Future Improvements

### **Potential Optimizations**

1. **Edge ML**: Run sign recognition in browser (TensorFlow.js)
2. **Caching**: Cache common translations locally
3. **WebSocket â†’ WebRTC**: Lower latency for predictions
4. **Service Worker**: Offline grammar/translation
5. **Backend Fix**: Improve spell-checker to recognize names

---

**This is your complete data flow! Every byte from camera to final output is now documented.** ğŸ“Šâœ¨

Let me know if you want me to dive deeper into any specific part! ğŸš€
